{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор размеченных текстов."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Оглавление:\n",
    "* [1. Подготовка](#1)\n",
    "* [2. Обучение](#2)\n",
    "* [3. Выводы](#3)\n",
    "* [Чек-лист проверки](#4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Подготовка <a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Импортируем библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <импорт библиотеки pandas>\n",
    "import pandas as pd\n",
    "\n",
    "# <импорт библиотеки sklearn>\n",
    "import sklearn\n",
    "\n",
    "# <Отключение предупреждений>\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# <импорт библиотеки numpy>\n",
    "import numpy as np\n",
    "\n",
    "# <импорт библиотеки tqdm, позволяющей отслеживать прогресс>\n",
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прочитаем файл с данными:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <чтение файла с данными с сохранением в переменную df>\n",
    "df = pd.read_csv('/datasets/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осмотрим данные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159571 entries, 0 to 159570\n",
      "Data columns (total 2 columns):\n",
      "text     159571 non-null object\n",
      "toxic    159571 non-null int64\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 2.4+ MB\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.info())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Столбец *text* содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Понизим регистр текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].astype(str).str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем лемматизацию текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# <импорт библиотеки nltk и ее лемматизатора>\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "\n",
    "# <импорт регулярных выражений>\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <создадим переменную корпуса текстов>\n",
    "corpus= df['text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим функцию лемматизации и функцию очистки текста:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    s = nltk.word_tokenize(text)\n",
    "    return ' '.join([lemmatizer.lemmatize(w) for w in s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем лемматизацию:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for i in range(len(df['text'])):\n",
    "    corpus[i] = lemmatize(clear_text(corpus[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для модели *BERT* сделаем свой датафрейм где текст только очистим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "corpus1 = df['text'].values\n",
    "for i in range(len(df['text'])):\n",
    "    corpus1[i] = clear_text(corpus1[i])\n",
    "    \n",
    "df1 = df\n",
    "df1['text'] = pd.Series(corpus1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датафрейм ниже для модели которая будет использовать *TF-IDF* матрицу как признаки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = pd.Series(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим данные на обучающую и тестовую выборку (мы будем использовать кросс-валидацию для оценки моделей):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(df, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы осмотрели данные, провели лемматизацию и очистку текста. В следующем пункте проведем апсэмплинг, создадим матрицу *TF-IDF* и попробуем обучить на ней модель. Также попробуем обучить другие модели на эмбеддингах BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Обучение <a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем апсэмплинг."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 114670\n",
      "1: 12986\n"
     ]
    }
   ],
   "source": [
    "print(\"0:\",train[train.toxic==0]['text'].count())\n",
    "print(\"1:\",train[train.toxic==1]['text'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    114670\n",
       "0    114670\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_majority = train[train.toxic==0]\n",
    "train_minority = train[train.toxic==1]\n",
    " \n",
    "# <апсэмплинг минорного класса>\n",
    "train_minority_upsampled = resample(train_minority, \n",
    "                                 replace=True,    \n",
    "                                 n_samples=114670,     \n",
    "                                 random_state=123) \n",
    " \n",
    "# <соединим минорный класс с мажоритарным>\n",
    "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
    "\n",
    "train_upsampled.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выделим признаки и целевой признак в обучающей и тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = train_upsampled['text']\n",
    "target_train = train_upsampled['toxic']\n",
    "features_test = test['text']\n",
    "target_test = test['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистим текст от стоп-слов и создадим матрицу *TF-IDF*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "stopwords = set(nltk_stopwords.words('english'))\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_tf = count_tf_idf.fit_transform(features_train)\n",
    "features_test_tf = count_tf_idf.transform(features_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем обучить логистическую регрессию на матрице *TF-IDF*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# <Импортируем метод логистической регрессии>\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# <Импортируем функцию cross_val_score>\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.4 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9583704953694141"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# <Создадим модель лог. регрессии>\n",
    "model_lr = LogisticRegression(class_weight = 'balanced', solver='liblinear', random_state=12345)\n",
    "\n",
    "# <Оценим качество модели, обученной в ходе перекрестной проверки>\n",
    "score = cross_val_score(model_lr,features_train_tf, target_train,cv=4,scoring='f1').mean()\n",
    "\n",
    "# <обучаем модель> \n",
    "model_lr.fit(features_train_tf, target_train)\n",
    "\n",
    "# <выведем долю правильных ответов>\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим переменные в памяти:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               train_upsampled: 88.7 MiB\n",
      "                features_train: 86.9 MiB\n",
      "                            df: 66.0 MiB\n",
      "                           df1: 66.0 MiB\n",
      "                         train: 53.9 MiB\n",
      "                train_majority: 49.5 MiB\n",
      "      train_minority_upsampled: 39.2 MiB\n",
      "                          test: 13.3 MiB\n",
      "                 features_test: 13.1 MiB\n",
      "                train_minority:  4.5 MiB\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    ''' by Fred Cirera,  https://stackoverflow.com/a/1094933/1870254, modified'''\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f %s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f %s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "for name, size in sorted(((name, sys.getsizeof(value)) for name, value in locals().items()),\n",
    "                         key= lambda x: -x[1])[:10]:\n",
    "    print(\"{:>30}: {:>8}\".format(name, sizeof_fmt(size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_upsampled\n",
    "del features_train\n",
    "del train\n",
    "del train_majority\n",
    "del train_minority_upsampled\n",
    "del features_test\n",
    "del train_minority\n",
    "del test\n",
    "del df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем использовать *BERT*. Очистим текст от стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['text'] = df1['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее импортируем библиотеки, используем видеокарту для вычислений. Создадим эмбеддинги."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = transformers.BertModel.from_pretrained('bert-base-uncased').to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = df1.sample(30000).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 26.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tokenized = sample['text'].apply(\n",
    "    lambda x: tokenizer.encode(x[:512], add_special_tokens=True))\n",
    "\n",
    "max_len = 0\n",
    "for i in tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "\n",
    "padded = np.array([i + [0]*(max_len - len(i)) for i in tokenized.values])\n",
    "\n",
    "attention_mask = np.where(padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cfac815e7504551b4f6599928776e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=600.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# <с большим batch size сталкивался с недостатком памяти видеокарты>\n",
    "batch_size = 50\n",
    "embeddings = []\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "        batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)]).to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "        \n",
    "        embeddings.append(batch_embeddings[0][:,0,:].cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим датафрейм, где эмбеддинги - признаки, также там будет присутствовать целевой признак."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Columns: 769 entries, 0 to toxic\n",
      "dtypes: float32(768), int64(1)\n",
      "memory usage: 88.1 MB\n"
     ]
    }
   ],
   "source": [
    "feature = np.concatenate(embeddings)\n",
    "df_bert = pd.DataFrame(feature)\n",
    "df_bert['toxic'] = sample['toxic']\n",
    "df_bert.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим выборки на обучающую и тестовую:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_bert, test_bert = train_test_split(df_bert, test_size=0.2, random_state=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем апсэмплиннг. Посчитаем количество текстов в кажом классе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: 21517\n",
      "1: 2483\n"
     ]
    }
   ],
   "source": [
    "print(\"0:\",train_bert[train_bert.toxic==0]['toxic'].count())\n",
    "print(\"1:\",train_bert[train_bert.toxic==1]['toxic'].count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    21517\n",
       "0    21517\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_majority = train_bert[train_bert.toxic==0]\n",
    "train_minority = train_bert[train_bert.toxic==1]\n",
    " \n",
    "# <даунсэмплинг мажоритарного класса>\n",
    "train_minority_upsampled = resample(train_minority, \n",
    "                                 replace=True,    \n",
    "                                 n_samples=21517,     \n",
    "                                 random_state=123) \n",
    " \n",
    "# <соединим минорный класс с мажоритарным>\n",
    "train_upsampled = pd.concat([train_minority_upsampled, train_majority])\n",
    "\n",
    "train_upsampled.toxic.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим наши выборки на признаки и целевой признак:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train_bert = train_upsampled.drop('toxic', axis=1)\n",
    "target_train_bert = train_upsampled['toxic']\n",
    "features_test_bert = test_bert.drop('toxic', axis=1)\n",
    "target_test_bert = test_bert['toxic']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалим ненужные переменные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test_bert\n",
    "del train_bert\n",
    "del df_bert\n",
    "del train_upsampled\n",
    "del train_majority\n",
    "del train_minority\n",
    "del train_minority_upsampled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучим модель логистической регрессии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 38s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8892766207337759"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# <Создадим модель лог. регрессии>\n",
    "model_lr_bert = LogisticRegression(solver='liblinear', random_state=12345)\n",
    "\n",
    "# <Оценим качество модели, обученной в ходе перекрестной проверки>\n",
    "score = cross_val_score(model_lr_bert,features_train_bert, target_train_bert,cv=4,scoring='f1').mean()\n",
    "\n",
    "# <обучаем модель> \n",
    "model_lr_bert.fit(features_train_bert, target_train_bert)\n",
    "\n",
    "# <выведем долю правильных ответов>\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем модель стохастического градиентного спуска:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 17 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8862954458101681"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "# <Создадим модель лог. регрессии>\n",
    "model_sgd = SGDClassifier(max_iter = 1000, n_jobs=-1)\n",
    "\n",
    "# <Оценим качество модели, обученной в ходе перекрестной проверки>\n",
    "score = cross_val_score(model_sgd,features_train_bert, target_train_bert,cv=4,scoring='f1').mean()\n",
    "\n",
    "# <обучаем модель> \n",
    "model_sgd.fit(features_train_bert, target_train_bert)\n",
    "\n",
    "# <выведем долю правильных ответов>\n",
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем использовать библиотеку *LightGBM* и ее *Dropouts meet Multiple Additive Regression Trees*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10]\ttraining's binary_error: 0.0738253\ttraining's binary_logloss: 0.410284\n",
      "[20]\ttraining's binary_error: 0.0501464\ttraining's binary_logloss: 0.281366\n",
      "[30]\ttraining's binary_error: 0.033764\ttraining's binary_logloss: 0.214823\n",
      "[40]\ttraining's binary_error: 0.0253985\ttraining's binary_logloss: 0.182563\n",
      "[50]\ttraining's binary_error: 0.022703\ttraining's binary_logloss: 0.177504\n",
      "[60]\ttraining's binary_error: 0.0183111\ttraining's binary_logloss: 0.159318\n",
      "[70]\ttraining's binary_error: 0.0159176\ttraining's binary_logloss: 0.147786\n",
      "[80]\ttraining's binary_error: 0.0141284\ttraining's binary_logloss: 0.13437\n",
      "[90]\ttraining's binary_error: 0.0131292\ttraining's binary_logloss: 0.137252\n",
      "[100]\ttraining's binary_error: 0.0118279\ttraining's binary_logloss: 0.127278\n",
      "Wall time: 47.5 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(boosting_type='dart', class_weight=None, colsample_bytree=1.0,\n",
       "               importance_type='split', learning_rate=0.1, max_depth=10,\n",
       "               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n",
       "               n_estimators=100, n_jobs=-1, num_iterations=100, num_leaves=75,\n",
       "               objective='binary', random_state=None, reg_alpha=0.0,\n",
       "               reg_lambda=0.0, silent=True, subsample=1.0,\n",
       "               subsample_for_bin=200000, subsample_freq=0, task='train')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "hyper_params = {\n",
    "    'learning_rate': 0.1,\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'dart',\n",
    "    'objective': 'binary',\n",
    "    \"num_iterations\": 100,\n",
    "    'max_depth': 10,\n",
    "    'num_leaves': 75\n",
    "}\n",
    "dart = lgb.LGBMClassifier(**hyper_params)\n",
    "dart.fit(features_train_bert, target_train_bert,\n",
    "        eval_set=[(features_train_bert, target_train_bert)],\n",
    "        eval_metric='binary_error',\n",
    "        early_stopping_rounds=10, verbose = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В следующем пункте будет видно, что вышли не самые хорошие результаты, скорее всего это связано с предобработкой данных. Хотя делал вроде все как надо. В целом было бы интересно узнать как получить на эмбеддингах *BERT* результаты лучше чем на матрице *TF-IDF*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Выводы <a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Лучший результат получился у модели логистической регрессии, где признаки - матрица *TF-IDF*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7497258771929824"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_lr.predict(features_test_tf)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(target_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У логистической регрессии на эмбеддингах *BERT* вышла худшая оценка, не совсем понятно почему."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5786802030456852"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_lr_bert.predict(features_test_bert)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(target_test_bert,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель стохастического градиентного спуска на эмбеддингах *BERT*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6325340246273493"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = model_sgd.predict(features_test_bert)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(target_test_bert,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель *Dropouts meet Multiple Additive Regression Trees* на эмбеддингах *BERT*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6428571428571429"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = dart.predict(features_test_bert)\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(target_test_bert,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Вывод"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получены не самые утешительные результаты. Есть предположение что стоит добавить еще один признак - размер фразы. Или какой либо еще.\n",
    "Здорово было узнать что видеокарта может здорово ускорить вычисление эмбеддингов."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
